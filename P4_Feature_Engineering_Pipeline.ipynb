{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/A_MLOps_Practives/blob/main/P4_Feature_Engineering_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ox94EiPnuYsM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Transform\n",
        "\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata, schema_utils\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "\n",
        "import tempfile\n",
        "import pprint\n",
        "import warnings\n",
        "\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "# ignore tf warning messages\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEZPjLXxugrU"
      },
      "outputs": [],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/492/metro+interstate+traffic+volume.zip\n",
        "!unzip \\*.zip && rm .zip\n",
        "import gzip\n",
        "with gzip.open('/content/Metro_Interstate_Traffic_Volume.csv.gz', 'rb') as f:\n",
        "    file_content = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RroAF5gjugo_"
      },
      "outputs": [],
      "source": [
        "# location of the pipeline metadata store\n",
        "_pipeline_root = './pipeline'\n",
        "\n",
        "# directory of the raw data files\n",
        "_data_root = './data'\n",
        "\n",
        "# path to the raw training data\n",
        "_data_filepath = os.path.join(_data_root, 'metro_traffic_volume.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbzqrDnku_Hf"
      },
      "outputs": [],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Preview the dataset\n",
        "!head {_data_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBk32Mku33x"
      },
      "source": [
        "#Create the Interactive Context\n",
        "When pushing to production, you want to automate the pipeline execution using orchestrators such as Apache Beam and Kubeflow. You will not be doing that just yet and will instead execute the pipeline from this notebook. When experimenting in a notebook environment, you will be manually executing the pipeline components (i.e. you are the orchestrator). For that, TFX provides the Interactive Context so you can step through each component and inspect its outputs.\n",
        "\n",
        "You will initialize the InteractiveContext below. This will create a database in the _pipeline_root directory which the different components will use to save or get the state of the component executions. You will learn more about this in Week 3 when we discuss ML Metadata. For now, you can think of it as the data store that makes it possible for the different pipeline components to work together.\n",
        "\n",
        "Note: You can configure the database to connect to but for this exercise, we will just use the default which is a newly created local sqlite file. You will see the warning after running the cell below and you can safely ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Btc8aoughT"
      },
      "outputs": [],
      "source": [
        "# Initialize the InteractiveContext with a local sqlite file.\n",
        "# If you leave `_pipeline_root` blank, then the db will be created in a temporary directory.\n",
        "# You can safely ignore the warning about the missing config file.\n",
        "context = InteractiveContext(pipeline_root=_pipeline_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yit8HLEXvDYs"
      },
      "source": [
        "#2 - Run TFX components interactively\n",
        "In the following exercises, you will create the data pipeline components one-by-one, run each of them, and visualize their output artifacts. Recall that we refer to the outputs of pipeline components as artifacts and these can be inputs to the next stage of the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS1e_bpVvOUR"
      },
      "source": [
        "##2.1 - ExampleGen\n",
        "The pipeline starts with the ExampleGen component. It will:\n",
        "\n",
        "split the data into training and evaluation sets (by default: 2/3 train, 1/3 eval).\n",
        "convert each data row into tf.train.Example format. This protocol buffer is designed for Tensorflow operations and is used by the TFX components.\n",
        "compress and save the data collection under the _pipeline_root directory for other components to access. These examples are stored in TFRecord format. This optimizes read and write operations within Tensorflow especially if you have a large collection of data.\n",
        "\n",
        "Exercise 1: ExampleGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSs_szeDu647"
      },
      "outputs": [],
      "source": [
        "# Instantiate ExampleGen with the input CSV dataset\n",
        "example_gen = tfx.components.CsvExampleGen(input_base=_data_root)\n",
        "\n",
        "# Run the component using the InteractiveContext instance\n",
        "context.run(example_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ze-JuTx5xk"
      },
      "source": [
        "You will notice that an output cell showing the execution results is automatically shown. This metadata is recorded into the database created earlier. This allows you to keep track of your project runs. For example, if you run it again, you will notice the .execution_id incrementing.\n",
        "\n",
        "The output of the components are called artifacts and you can see an example by navigating through .component.outputs > ['examples'] > Channel > ._artifacts > [0] above. It shows information such as where the converted data is stored (.uri) and the splits generated (.split_names).\n",
        "\n",
        "You can also examine the output artifacts programmatically with the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsd8jsju60V"
      },
      "outputs": [],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "try:\n",
        "    # get the artifact object\n",
        "    artifact = example_gen.outputs['examples'].get()[0]\n",
        "\n",
        "    # print split names and uri\n",
        "    print(f'split names: {artifact.split_names}')\n",
        "    print(f'artifact uri: {artifact.uri}')\n",
        "\n",
        "# for grading since context.run() does not work outside the notebook\n",
        "except IndexError:\n",
        "    print(\"context.run() was no-op\")\n",
        "    examples_path = './pipeline/CsvExampleGen/examples'\n",
        "    dir_id = os.listdir(examples_path)[0]\n",
        "    artifact_uri = f'{examples_path}/{dir_id}'\n",
        "\n",
        "else:\n",
        "    artifact_uri = artifact.uri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFwh99uwyXQH"
      },
      "source": [
        "If you're wondering , the number in ./pipeline/CsvExampleGen/examples/{number} is the execution id associated with that dataset. If you restart the kernel of this workspace and re-run up to this cell, you will notice a new folder with a different id name created. This shows that TFX is keeping versions of your data so you can roll back if you want to investigate a particular execution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp4TFje_y3GK"
      },
      "source": [
        "In a notebook environment, it may be useful to examine a few examples of the data especially if you're still experimenting. Since the data collection is saved in TFRecord format, you will need to use methods that work with that data type. You will need to unpack the individual examples from the TFRecord file and format it for printing. Let's do that in the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrD3SE-Pu6uA"
      },
      "outputs": [],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Get the URI of the output artifact representing the training examples, which is a directory\n",
        "train_uri = os.path.join(artifact_uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYwWqM6Ky8LA"
      },
      "outputs": [],
      "source": [
        "# Define a helper function to get individual examples\n",
        "def get_records(dataset, num_records):\n",
        "    '''Extracts records from the given dataset.\n",
        "    Args:\n",
        "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
        "        num_records (int): number of records to preview\n",
        "    '''\n",
        "\n",
        "    # initialize an empty list\n",
        "    records = []\n",
        "\n",
        "    # Use the `take()` method to specify how many records to get\n",
        "    for tfrecord in dataset.take(num_records):\n",
        "\n",
        "        # Get the numpy property of the tensor\n",
        "        serialized_example = tfrecord.numpy()\n",
        "\n",
        "        # Initialize a `tf.train.Example()` to read the serialized data\n",
        "        example = tf.train.Example()\n",
        "\n",
        "        # Read the example data (output is a protocol buffer message)\n",
        "        example.ParseFromString(serialized_example)\n",
        "\n",
        "        # convert the protocol bufffer message to a Python dictionary\n",
        "        example_dict = (MessageToDict(example))\n",
        "\n",
        "        # append to the records list\n",
        "        records.append(example_dict)\n",
        "\n",
        "    return records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn-FWhsmy8I0"
      },
      "outputs": [],
      "source": [
        "# Get 3 records from the dataset\n",
        "sample_records = get_records(dataset, 3)\n",
        "\n",
        "# Print the output\n",
        "pp.pprint(sample_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2HQDI2b0S_5"
      },
      "source": [
        "Now that `ExampleGen` has finished ingesting the data, the next step is data analysis.\n",
        "\n",
        "\n",
        "##2.2 - StatisticsGen\n",
        "The StatisticsGen component computes statistics over your dataset for data analysis, as well as for use in downstream components. It uses the TensorFlow Data Validation library.\n",
        "\n",
        "StatisticsGen takes as input the dataset ingested using CsvExampleGen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL2w7znmy8Gc"
      },
      "outputs": [],
      "source": [
        "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
        "statistics_gen = tfx.components.StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "# Execute the component\n",
        "context.run(statistics_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wdU_Llx1K7-"
      },
      "outputs": [],
      "source": [
        "# Show the output statistics\n",
        "context.show(statistics_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_-xp5eS7Pkt"
      },
      "source": [
        "2.3 - SchemaGen\n",
        "The SchemaGen component also uses TFDV to generate a schema based on your data statistics. As you've learned previously, a schema defines the expected bounds, types, and properties of the features in your dataset.\n",
        "\n",
        "SchemaGen will take as input the statistics that we generated with StatisticsGen, looking at the training split by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBSAQ24j7Q_4"
      },
      "outputs": [],
      "source": [
        "# Instantiate SchemaGen with the StatisticsGen ingested dataset\n",
        "schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    )\n",
        "\n",
        "# Run the component\n",
        "context.run(schema_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDWjRoxDRTJO"
      },
      "source": [
        "##2.4 - ExampleValidator\n",
        "The ExampleValidator component detects anomalies in your data based on the generated schema from the previous step. Like the previous two components, it also uses TFDV under the hood.\n",
        "\n",
        "ExampleValidator will take as input the statistics from StatisticsGen and the schema from SchemaGen. By default, it compares the statistics from the evaluation split to the schema from the training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LtbyThO7z-3"
      },
      "outputs": [],
      "source": [
        "# Instantiate ExampleValidator with the StatisticsGen and SchemaGen ingested data\n",
        "example_validator = tfx.components.ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "\n",
        "# Run the component.\n",
        "context.run(example_validator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP0m6XSlSGFK"
      },
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "context.show(example_validator.outputs['anomalies'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWdOgdJrSMpu"
      },
      "source": [
        "##2.5 - Transform\n",
        "The Transform component performs feature engineering for both training and serving datasets. It uses the TensorFlow Transform library introduced in the first ungraded lab of this week.\n",
        "\n",
        "Transform will take as input the data from ExampleGen, the schema from SchemaGen, as well as a module containing the preprocessing function.\n",
        "\n",
        "In this section, you will work on an example of a user-defined Transform code. The pipeline needs to load this as a module so you need to use the magic command %% writefile to save the file to disk. Let's first define a few constants that group the data's attributes according to the transforms we will perform later. This file will also be saved locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU1SD1NJSF_n"
      },
      "outputs": [],
      "source": [
        "# Set the constants module filename\n",
        "_traffic_constants_module_file = 'traffic_constants.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlYlM2znSdnk"
      },
      "outputs": [],
      "source": [
        "%%writefile {_traffic_constants_module_file}\n",
        "\n",
        "# Features to be scaled to the z-score\n",
        "DENSE_FLOAT_FEATURE_KEYS = ['temp', 'snow_1h']\n",
        "\n",
        "# Features to bucketize\n",
        "BUCKET_FEATURE_KEYS = ['rain_1h']\n",
        "\n",
        "# Number of buckets used by tf.transform for encoding each feature.\n",
        "FEATURE_BUCKET_COUNT = {'rain_1h': 3}\n",
        "\n",
        "# Feature to scale from 0 to 1\n",
        "RANGE_FEATURE_KEYS = ['clouds_all']\n",
        "\n",
        "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
        "OOV_SIZE = 10\n",
        "\n",
        "# Features with string data types that will be converted to indices\n",
        "VOCAB_FEATURE_KEYS = [\n",
        "    'holiday',\n",
        "    'weather_main',\n",
        "    'weather_description'\n",
        "]\n",
        "\n",
        "# Features with int data type that will be kept as is\n",
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'hour', 'day', 'day_of_week', 'month'\n",
        "]\n",
        "\n",
        "# Feature to predict\n",
        "VOLUME_KEY = 'traffic_volume'\n",
        "\n",
        "def transformed_name(key):\n",
        "    return key + '_xf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbSsHo1BTDxz"
      },
      "source": [
        "Next, you will fill out the transform module. As mentioned, this will also be saved to disk. Specifically, you will complete the preprocessing_fn which defines the transformations. See the code comments for instructions and refer to the tft module documentation to look up which function to use for a given group of keys.\n",
        "\n",
        "For the label (i.e. VOLUME_KEY), you will transform it to indicate if it is greater than the mean of the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61nJBYxESeRJ"
      },
      "outputs": [],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Set the transform module filename\n",
        "_traffic_transform_module_file = 'traffic_transform.py'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5K+1nbwcyH/LkjFkRDdoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
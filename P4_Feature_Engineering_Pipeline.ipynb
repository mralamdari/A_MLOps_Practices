{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs4Lx7C4fuxKA5VB7htjKP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/A_MLOps_Practives/blob/main/P4_Feature_Engineering_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ox94EiPnuYsM"
      },
      "outputs": [],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata, schema_utils\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "\n",
        "import tempfile\n",
        "import pprint\n",
        "import warnings\n",
        "\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "# ignore tf warning messages\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/492/metro+interstate+traffic+volume.zip"
      ],
      "metadata": {
        "id": "zEZPjLXxugrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# location of the pipeline metadata store\n",
        "_pipeline_root = './pipeline'\n",
        "\n",
        "# directory of the raw data files\n",
        "_data_root = './data'\n",
        "\n",
        "# path to the raw training data\n",
        "_data_filepath = os.path.join(_data_root, 'metro_traffic_volume.csv')"
      ],
      "metadata": {
        "id": "RroAF5gjugo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Preview the dataset\n",
        "!head {_data_filepath}"
      ],
      "metadata": {
        "id": "xbzqrDnku_Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Interactive Context\n",
        "When pushing to production, you want to automate the pipeline execution using orchestrators such as Apache Beam and Kubeflow. You will not be doing that just yet and will instead execute the pipeline from this notebook. When experimenting in a notebook environment, you will be manually executing the pipeline components (i.e. you are the orchestrator). For that, TFX provides the Interactive Context so you can step through each component and inspect its outputs.\n",
        "\n",
        "You will initialize the InteractiveContext below. This will create a database in the _pipeline_root directory which the different components will use to save or get the state of the component executions. You will learn more about this in Week 3 when we discuss ML Metadata. For now, you can think of it as the data store that makes it possible for the different pipeline components to work together.\n",
        "\n",
        "Note: You can configure the database to connect to but for this exercise, we will just use the default which is a newly created local sqlite file. You will see the warning after running the cell below and you can safely ignore it."
      ],
      "metadata": {
        "id": "pzBk32Mku33x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the InteractiveContext with a local sqlite file.\n",
        "# If you leave `_pipeline_root` blank, then the db will be created in a temporary directory.\n",
        "# You can safely ignore the warning about the missing config file.\n",
        "context = InteractiveContext(pipeline_root=_pipeline_root)"
      ],
      "metadata": {
        "id": "-_Btc8aoughT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 - Run TFX components interactively\n",
        "In the following exercises, you will create the data pipeline components one-by-one, run each of them, and visualize their output artifacts. Recall that we refer to the outputs of pipeline components as artifacts and these can be inputs to the next stage of the pipeline."
      ],
      "metadata": {
        "id": "Yit8HLEXvDYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 - ExampleGen\n",
        "The pipeline starts with the ExampleGen component. It will:\n",
        "\n",
        "split the data into training and evaluation sets (by default: 2/3 train, 1/3 eval).\n",
        "convert each data row into tf.train.Example format. This protocol buffer is designed for Tensorflow operations and is used by the TFX components.\n",
        "compress and save the data collection under the _pipeline_root directory for other components to access. These examples are stored in TFRecord format. This optimizes read and write operations within Tensorflow especially if you have a large collection of data.\n",
        "\n",
        "Exercise 1: ExampleGen"
      ],
      "metadata": {
        "id": "tS1e_bpVvOUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate ExampleGen with the input CSV dataset\n",
        "example_gen = tfx.components.CsvExampleGen(input_base=_data_root)\n",
        "\n",
        "# Run the component using the InteractiveContext instance\n",
        "context.run(example_gen)"
      ],
      "metadata": {
        "id": "SSs_szeDu647"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice that an output cell showing the execution results is automatically shown. This metadata is recorded into the database created earlier. This allows you to keep track of your project runs. For example, if you run it again, you will notice the .execution_id incrementing.\n",
        "\n",
        "The output of the components are called artifacts and you can see an example by navigating through .component.outputs > ['examples'] > Channel > ._artifacts > [0] above. It shows information such as where the converted data is stored (.uri) and the splits generated (.split_names).\n",
        "\n",
        "You can also examine the output artifacts programmatically with the code below."
      ],
      "metadata": {
        "id": "O3ze-JuTx5xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "try:\n",
        "    # get the artifact object\n",
        "    artifact = example_gen.outputs['examples'].get()[0]\n",
        "\n",
        "    # print split names and uri\n",
        "    print(f'split names: {artifact.split_names}')\n",
        "    print(f'artifact uri: {artifact.uri}')\n",
        "\n",
        "# for grading since context.run() does not work outside the notebook\n",
        "except IndexError:\n",
        "    print(\"context.run() was no-op\")\n",
        "    examples_path = './pipeline/CsvExampleGen/examples'\n",
        "    dir_id = os.listdir(examples_path)[0]\n",
        "    artifact_uri = f'{examples_path}/{dir_id}'\n",
        "\n",
        "else:\n",
        "    artifact_uri = artifact.uri"
      ],
      "metadata": {
        "id": "iRsd8jsju60V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're wondering , the number in ./pipeline/CsvExampleGen/examples/{number} is the execution id associated with that dataset. If you restart the kernel of this workspace and re-run up to this cell, you will notice a new folder with a different id name created. This shows that TFX is keeping versions of your data so you can roll back if you want to investigate a particular execution.\n"
      ],
      "metadata": {
        "id": "YFwh99uwyXQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a notebook environment, it may be useful to examine a few examples of the data especially if you're still experimenting. Since the data collection is saved in TFRecord format, you will need to use methods that work with that data type. You will need to unpack the individual examples from the TFRecord file and format it for printing. Let's do that in the following cells:"
      ],
      "metadata": {
        "id": "Mp4TFje_y3GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Get the URI of the output artifact representing the training examples, which is a directory\n",
        "train_uri = os.path.join(artifact_uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
      ],
      "metadata": {
        "id": "YrD3SE-Pu6uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a helper function to get individual examples\n",
        "def get_records(dataset, num_records):\n",
        "    '''Extracts records from the given dataset.\n",
        "    Args:\n",
        "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
        "        num_records (int): number of records to preview\n",
        "    '''\n",
        "\n",
        "    # initialize an empty list\n",
        "    records = []\n",
        "\n",
        "    # Use the `take()` method to specify how many records to get\n",
        "    for tfrecord in dataset.take(num_records):\n",
        "\n",
        "        # Get the numpy property of the tensor\n",
        "        serialized_example = tfrecord.numpy()\n",
        "\n",
        "        # Initialize a `tf.train.Example()` to read the serialized data\n",
        "        example = tf.train.Example()\n",
        "\n",
        "        # Read the example data (output is a protocol buffer message)\n",
        "        example.ParseFromString(serialized_example)\n",
        "\n",
        "        # convert the protocol bufffer message to a Python dictionary\n",
        "        example_dict = (MessageToDict(example))\n",
        "\n",
        "        # append to the records list\n",
        "        records.append(example_dict)\n",
        "\n",
        "    return records"
      ],
      "metadata": {
        "id": "DYwWqM6Ky8LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 3 records from the dataset\n",
        "sample_records = get_records(dataset, 3)\n",
        "\n",
        "# Print the output\n",
        "pp.pprint(sample_records)"
      ],
      "metadata": {
        "id": "rn-FWhsmy8I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that `ExampleGen` has finished ingesting the data, the next step is data analysis.\n",
        "\n",
        "\n",
        "##2.2 - StatisticsGen\n",
        "The StatisticsGen component computes statistics over your dataset for data analysis, as well as for use in downstream components. It uses the TensorFlow Data Validation library.\n",
        "\n",
        "StatisticsGen takes as input the dataset ingested using CsvExampleGen."
      ],
      "metadata": {
        "id": "N2HQDI2b0S_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
        "statistics_gen = tfx.components.StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "# Execute the component\n",
        "context.run(statistics_gen)"
      ],
      "metadata": {
        "id": "QL2w7znmy8Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the output statistics\n",
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "metadata": {
        "id": "_wdU_Llx1K7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
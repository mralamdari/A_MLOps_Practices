{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx75KxbMNvf2znz+XOqhyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/A_MLOps_Practives/blob/main/p3_TFDV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "# grader-required-cell\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tempfile, urllib, zipfile\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from tensorflow_data_validation.utils import slicing_util\n",
        "from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList, DatasetFeatureStatistics\n",
        "\n",
        "# Set TF's logger to only display errors to avoid internal warnings being shown\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "devkJBi5GO3e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-data-validation\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "CGchAVAqrsrZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKnZP8xXNBtn",
        "outputId": "81c6fb53-df96-43f7-90bb-15bbb93a7c80"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from util import add_extra_rows\n",
        "\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "print('TFDV Version: {}'.format(tfdv.__version__))\n",
        "print('Tensorflow Version: {}'.format(tf.__version__))"
      ],
      "metadata": {
        "id": "DVLt1NSGpVex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKsl4v9kompD"
      },
      "outputs": [],
      "source": [
        "https://archive.ics.uci.edu/static/public/296/diabetes+130-us+hospitals+for+years+1999-2008.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/diabetic_data.csv', header=0, na_values = '?')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "p-WlUxWtffuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "def prepare_data_splits_from_dataframe(df):\n",
        "    '''\n",
        "    Splits a Pandas Dataframe into training, evaluation and serving sets.\n",
        "\n",
        "    Parameters:\n",
        "            df : pandas dataframe to split\n",
        "\n",
        "    Returns:\n",
        "            train_df: Training dataframe(70% of the entire dataset)\n",
        "            eval_df: Evaluation dataframe (15% of the entire dataset)\n",
        "            serving_df: Serving dataframe (15% of the entire dataset, label column dropped)\n",
        "    '''\n",
        "\n",
        "    # 70% of records for generating the training set\n",
        "    train_len = int(len(df) * 0.7)\n",
        "\n",
        "    # Remaining 30% of records for generating the evaluation and serving sets\n",
        "    eval_serv_len = len(df) - train_len\n",
        "\n",
        "    # Half of the 30%, which makes up 15% of total records, for generating the evaluation set\n",
        "    eval_len = eval_serv_len // 2\n",
        "\n",
        "    # Remaining 15% of total records for generating the serving set\n",
        "    serv_len = eval_serv_len - eval_len\n",
        "\n",
        "    # Split the dataframe into the three subsets\n",
        "    train_df = df.iloc[:train_len].reset_index(drop=True)\n",
        "    eval_df = df.iloc[train_len: train_len + eval_len].reset_index(drop=True)\n",
        "    serving_df = df.iloc[train_len + eval_len: train_len + eval_len + serv_len].reset_index(drop=True)\n",
        "\n",
        "    # Serving data emulates the data that would be submitted for predictions, so it should not have the label column.\n",
        "    serving_df = serving_df.drop(['readmitted'], axis=1)\n",
        "\n",
        "    return train_df, eval_df, serving_df"
      ],
      "metadata": {
        "id": "5tO24Xtgfj3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Split the datasets\n",
        "train_df, eval_df, serving_df = prepare_data_splits_from_dataframe(df)\n",
        "print('Training dataset has {} records\\nValidation dataset has {} records\\nServing dataset has {} records'.format(len(train_df),len(eval_df),len(serving_df)))"
      ],
      "metadata": {
        "id": "w3wq-GhwgTcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Define features to remove\n",
        "features_to_remove = {'encounter_id', 'patient_nbr'}\n",
        "\n",
        "# Collect features to include while computing the statistics\n",
        "approved_cols = [col for col in df.columns if (col not in features_to_remove)]\n",
        "\n",
        "# Instantiate a StatsOptions class and define the feature_allowlist property\n",
        "stats_options = tfdv.StatsOptions(feature_allowlist=approved_cols)\n",
        "\n",
        "# Review the features to generate the statistics\n",
        "for feature in stats_options.feature_allowlist:\n",
        "    print(feature)"
      ],
      "metadata": {
        "id": "6Fol2Gu0gZzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "### START CODE HERE\n",
        "train_stats = tfdv.generate_statistics_from_dataframe(train_df)\n",
        "### END CODE HERE"
      ],
      "metadata": {
        "id": "8Uc5kzZ7gZwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# TEST CODE\n",
        "\n",
        "# get the number of features used to compute statistics\n",
        "print(f\"Number of features used: {len(train_stats.datasets[0].features)}\")\n",
        "\n",
        "# check the number of examples used\n",
        "print(f\"Number of examples used: {train_stats.datasets[0].num_examples}\")\n",
        "\n",
        "# check the column names of the first and last feature\n",
        "print(f\"First feature: {train_stats.datasets[0].features[0].path.step[0]}\")\n",
        "print(f\"Last feature: {train_stats.datasets[0].features[-1].path.step[0]}\")"
      ],
      "metadata": {
        "id": "2G2zttCliRuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfdv.visualize_statistics(train_stats)"
      ],
      "metadata": {
        "id": "OEhTvkK-iTHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Infer the data schema by using the training statistics that you generated\n",
        "schema = tfdv.infer_schema(statistics=train_stats)\n",
        "\n",
        "# Display the data schema\n",
        "tfdv.display_schema(schema)"
      ],
      "metadata": {
        "id": "rrMFaDnougta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of features\n",
        "print(f\"Number of features in schema: {len(schema.feature)}\")\n",
        "\n",
        "# Check domain name of 2nd feature\n",
        "print(f\"Second feature in schema: {list(schema.feature)[1].domain}\")"
      ],
      "metadata": {
        "id": "oeHAuRr_vChE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Generate evaluation dataset statistics\n",
        "# HINT: Remember to use the evaluation dataframe and to pass the stats_options (that you defined before) as an argument\n",
        "eval_stats = tfdv.generate_statistics_from_dataframe(eval_df)\n",
        "# Compare evaluation data with training data\n",
        "# HINT: Remember to use both the evaluation and training statistics with the lhs_statistics and rhs_statistics arguments\n",
        "# HINT: Assign the names of 'EVAL_DATASET' and 'TRAIN_DATASET' to the lhs and rhs protocols\n",
        "\n",
        "tfdv.visualize_statistics(lhs_statistics=eval_stats,\n",
        "                          rhs_statistics=train_stats,\n",
        "                          lhs_name='EVAL_DATASET',\n",
        "                          rhs_name='TRAIN_DATASET')"
      ],
      "metadata": {
        "id": "fEl_RmBJvCeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# get the number of features used to compute statistics\n",
        "print(f\"Number of features: {len(eval_stats.datasets[0].features)}\")\n",
        "\n",
        "# check the number of examples used\n",
        "print(f\"Number of examples: {eval_stats.datasets[0].num_examples}\")\n",
        "\n",
        "# check the column names of the first and last feature\n",
        "print(f\"First feature: {eval_stats.datasets[0].features[0].path.step[0]}\")\n",
        "print(f\"Last feature: {eval_stats.datasets[0].features[-1].path.step[0]}\")"
      ],
      "metadata": {
        "id": "zkX5f-OJvCbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "def calculate_and_display_anomalies(statistics, schema):\n",
        "    '''\n",
        "    Calculate and display anomalies.\n",
        "\n",
        "            Parameters:\n",
        "                    statistics : Data statistics in statistics_pb2.DatasetFeatureStatisticsList format\n",
        "                    schema : Data schema in schema_pb2.Schema format\n",
        "\n",
        "            Returns:\n",
        "                    display of calculated anomalies\n",
        "    '''\n",
        "    ### START CODE HERE\n",
        "    # HINTS: Pass the statistics and schema parameters into the validation function\n",
        "    anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\n",
        "\n",
        "    # HINTS: Display input anomalies by using the calculated anomalies\n",
        "    tfdv.display_anomalies(anomalies)\n",
        "\n",
        "calculate_and_display_anomalies(eval_stats, schema=schema)"
      ],
      "metadata": {
        "id": "6j_L2BzS0_Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 6: Fix evaluation anomalies in the schema\n",
        "The evaluation data has records with values for the features glimepiride-pioglitazone and medical_speciality that were not included in the schema generated from the training data. You can fix this by adding the new values that exist in the evaluation dataset to the domain of these features.\n",
        "\n",
        "To get the domain of a particular feature you can use tfdv.get_domain()."
      ],
      "metadata": {
        "id": "C8W2U3Kq5zB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the domain associated with the input feature, glimepiride-pioglitazone, from the schema\n",
        "glimepiride_pioglitazone_domain = tfdv.get_domain(schema, 'glimepiride-pioglitazone')\n",
        "\n",
        "# HINT: Append the missing value 'Steady' to the domain\n",
        "glimepiride_pioglitazone_domain.value.append('Steady')\n",
        "\n",
        "# Get the domain associated with the input feature, medical_specialty, from the schema\n",
        "medical_specialty_domain = tfdv.get_domain(schema, 'medical_specialty')\n",
        "\n",
        "# HINT: Append the missing value 'Neurophysiology' to the domain\n",
        "medical_specialty_domain.value.append('Neurophysiology')\n",
        "\n",
        "# HINT: Re-calculate and re-display anomalies with the new schema\n",
        "calculate_and_display_anomalies(eval_stats, schema=schema)\n",
        "### END CODE HERE"
      ],
      "metadata": {
        "id": "hlOVUy3P0_I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "28gwQujl0_Dd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}